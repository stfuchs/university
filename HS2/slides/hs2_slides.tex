\documentclass{beamer}
\usetheme{default}
\useoutertheme{infolines}
\setbeamertemplate{navigation symbols}{}

\usepackage{amsmath}
\usepackage{tikz}
\usetikzlibrary{arrows}

\usepackage[absolute,overlay]{textpos}
\newenvironment{reference}[2]{%
  \begin{textblock*}{\textwidth}(#1,#2)
      \footnotesize\it\bgroup\color{red!50!black}}{\egroup\end{textblock*}}

\newcommand{\D}{\mathcal{D}}
\newcommand{\ly}{\hat{y}(x;\D)}
\newcommand{\xt}{\tilde{x}}
\newcommand{\yt}{\tilde{y}}
\newcommand{\hht}{\tilde{h}}
\newcommand{\st}{\tilde{\sigma}}
\newcommand{\mut}{\tilde{\mu}}
\newcommand{\E}[1]{\left< #1 \right>}

% predefined font sizes:
% \tiny, \scriptsize, \footnotesize, \small, \normalsize (default),
% \large, \Large, \LARGE, \huge, \Huge
\begin{document}

\title{Active Learning with Statistical Models}
\subtitle{D. A. Cohn, Z. Ghahramani, M. I. Jordan}

\institute[]{Hauptseminar Machine Learning (WS 13/14)\\
Steffen Fuchs}
\author[S. Fuchs]{}

\date[15.01.2014]{January 15, 2014}

%%%_____________________________________________________________________________
%%%
%%%  title
%%%_____________________________________________________________________________
\begin{frame}[plain]
  \titlepage
\end{frame}

%%%_____________________________________________________________________________
%%%
%%%  overview
%%%_____________________________________________________________________________
\begin{frame}{Overview}
\begin{itemize}
  \item What is Active Learning?
  \item Active Learning - A Statistical Approach
  \item Example 1 - Gaussian Mixture Model
  \item Example 2 - Locally Weighted Regression
  \item Experimental Results
\end{itemize}
\end{frame}

%%%_____________________________________________________________________________
%%%
%%%  intro
%%%_____________________________________________________________________________
\begin{frame}{Introduction}
What is Active Learning and why would we need it?
\begin{itemize}
  \item Most of the time learners are treated as passive recipients of data
  \item In many situations a learner also has the ability to act, to gather data
    and to influence the world it is trying to understand
  \item Active learning is the study of how to use this ability effectively
\end{itemize}

\begin{center}\begin{tikzpicture}[
  box/.style={rectangle,rounded corners,draw=black!80,thick,
    inner sep=5pt, minimum width=2.5cm, node distance=2cm},
  to/.style={->, >=stealth', shorten >=2pt},
  every node/.style={align=center}]

  \node[box,fill=green!20] (env) {Environment};
  \node[box,fill=blue!20] (mod)[below of=env] {Learner};

  \path (mod.west)
        edge[to,bend left=45] node[left]{take action} (env.west);
  \path (env.east)
        edge[to,bend left=45] node[right]{add outcome to \\ training set} (mod.east);
  %\draw (env) -- node[right]{add outcome to training set} -- (mod);
\end{tikzpicture}\end{center}
\end{frame}

%..............................................................................
\begin{frame}{Introduction}
Active learning offers its greatest rewards when
\begin{itemize}
  \item data is expensive or difficult to obtain
  \item the environment is complex or dangerous
\end{itemize}
Examples include
\begin{itemize}
  \item query human experts for labeling (natural language understanding,
    biomedical information extraction)
  \item selecting locations for sensor measurments to locate buried hazardous wastes
  \item selecting joint angles or torques to learn kinematics or dynamcis of a robot
\end{itemize}
\end{frame}

%%%_____________________________________________________________________________
%%%
%%%  GENERAL
%%%_____________________________________________________________________________
\begin{frame}{Active Learning - A Statistical Approach}
We define
\begin{itemize}
\item $P(x,y)$ as the unknown joint distribution
\item $P(x)$ the known marginal distribution of input $x$ and
\item $\ly$ as the learner's output for $x$, given training set $\D$
\end{itemize}
The expected error of the learner can be written as
\begin{equation}
    \int_x E_T \left[\left(\ly - y(x)\right)^2|x\right] P(x)dx
\end{equation}
where $E_T\left[ \cdot \right]$ denotes the expectation over $P(y|x)$ and $\D$.
\end{frame}

%..............................................................................
\begin{frame}{Active Learning - A Statistical Approach}
\begin{reference}{4mm}{85mm}
  S. Geman, E. Bienenstock and R. Doursat,
  Neural networks and the bias/variance dilemma, Neural computation 1992
\end{reference}

This expecation can be decomposed as
\small \begin{align}
    E_T \left[\left(\ly - y(x)\right)^2|x\right] =
    & \; E\left[\left(y(x) - E\left[y|x\right]\right)^2\right] \tag{2.1}\\
    & + \left( E_{\D} \left[\ly\right] - E\left[y|x\right]\right)^2 \tag{2.2}\\
    & + E_{\D} \left[\left(\ly - E_{\D} \left[\ly\right]\right)^2 \right] \tag{2.3}
\end{align}\normalsize
where
\begin{columns}[t]
\begin{column}{0.02\textwidth} \end{column} %empty
\begin{column}{0.98\textwidth}\begin{itemize}
  \item[{\small(2.1)}] is the variance of the distribution, not depending on the learner
  \item[{\small(2.2)}] is the squared bias of the learner(here: assumed to be zero)
  \item[{\small(2.3)}] is the learner's variance
\end{itemize}\end{column}
\end{columns}
\end{frame}

%..............................................................................
\begin{frame}{Active Learning - A Statistical Approach}
Thus, in order to minimize the learner's error, the only part we are interested
in is its variance
\begin{equation}\begin{split}
  \sigma_{\hat{y}}^2
  & = E_{\D} \left[\left(\ly - E_{\D} \left[\ly\right]\right)^2 \right] \\
  & = \left<(\hat{y} - \left<\hat{y}\right> )^2\right>
\end{split}\end{equation}
depending on $x$ and $\D$, where $\left< \cdot \right>$ denotes
$E_\D\left[\cdot\right]$.\\

For a new query at $\xt$, the resulting $(\xt,\yt)$ is added to $\D$, which
changes the expectation of the learner's variance
\begin{equation}
  \E{\st_{\hat{y}}^2} = E_{\D\cup(\xt,\yt)} \left[\sigma_{\hat{y}}^2 | \xt \right]
\end{equation}
\end{frame}

%..............................................................................
\begin{frame}{Active Learning - A Statistical Approach}
The goal is to select data in a way to minimize the value of the
learner's variance intergraded over X.
\end{frame}

%%%_____________________________________________________________________________
%%%
%%%  Gaussian Mixture Model
%%%_____________________________________________________________________________

\begin{frame}{Gaussian Mixture Model (GMM)}
!!!IMAGE!!!
\begin{equation}
P(x,y|i) = \frac{1}{2\pi\sqrt{|\Sigma_i|}} \exp
\left[ -\frac{1}{2}(\mathbf{x}-\mu_i)^T\Sigma_i^{-1}(\mathbf{x}-\mu_i) \right]
\end{equation}
with
\[
\mathbf{x} = \left[ \begin{array}{c} x \\ y \end{array} \right] \quad
\mu_i = \left[ \begin{array}{c} \mu_{x,i} \\ \mu_{x,i} \end{array} \right] \quad
\Sigma_i = \left[ \begin{array}{cc} \sigma^2_{x,i} & \sigma_{xy,i} \\
    \sigma_{xy,i} & \sigma^2_{y,i} \end{array} \right] \quad
\]
obtained from an EM algorithm.
\end{frame}

%..............................................................................
\begin{frame}{Gaussian Mixture Model (GMM)}
\footnotesize
By using these parameters one can compute the learner's new variance at $x$ in
closed form as
\begin{equation}
   \E{\st_{\hat{y}}^2} = \sum_{i=1}^N
   \frac{h_i^2 \E{\st_{y|x,i}^2}}{ (n_i+\hht_i)^2}
   \left( 1+\frac{(x-\mu_{x,i})^2}{\sigma_{x,i}^2} \right)
\end{equation}
with
\tiny
\begin{columns}[t]
\begin{column}{0.45\textwidth}
\[\begin{split}
\E{\st_{y|x,i}^2} &= \E{\st_{y,i}^2} - \frac{\E{\st_{xy,i}^2}}{\sigma_{x,i}^2},\\
\E{\st_{xy,i}^2} &= \E{\st_{xy,i}}^2 +
\frac{n_i^2\hht_i^2\sigma_{y|\xt,i}^2(\xt-\mu_{x,i})^2}{(n_i+\hht_i)^4},\\
\sigma_{y|x,i}^2 &= \sigma_{y,i}^2 - \frac{\sigma_{xy,i}^2}{\sigma_{x,i}^2},\\
h_i &\equiv h_i(x) = \frac{P(x|i)}{\sum_{j=1}^N P(x|j)},
\end{split}\]
\end{column}
\begin{column}{0.55\textwidth}
\[\begin{split}
\E{\st_{y,i}^2} &= \frac{n_i\sigma_{y,i}^2}{n_i+\hht_i} +
\frac{n_i\hht_i\left(\sigma_{y|\xt,i}^2 + (\hat{y}_i(\xt)-\mu_{y,i})^2 \right)}
{(n_i+\hht_i)^2}, \\
\E{\st_{xy,i}} &= \frac{n_i\sigma_{xy,i}}{n_i+\hht_i} +
\frac{n_i\hht_i (\xt-\mu_{x,i})(\hat{y}_i(\xt)-\mu_{y,i})}
{(n_i+\hht_i)^2},\\
\hat{y}_i(x) &= \mu_{y,i} + \frac{\sigma_{xy,i}}{\sigma_{x,i}^2}(x-\mu_{x,i}), \\
n_i &= \sum_{j=1}^m \frac{P(x_j,y_j|i)}{\sum_{k=1}^N P(x_j,y_j|k)}
\end{split}\]
\end{column}
\end{columns}
\end{frame}

%%%_____________________________________________________________________________
%%%
%%%  Locally weighted regression
%%%_____________________________________________________________________________

\begin{frame}{Locally Weighted Regression (LWR)}
!!!IMAGE!!!

where a Gaussian
\begin{equation}
h_i(x) \equiv h(x-x_i) = \exp(-k(x-x_i)^2)
\end{equation}
with smoothing parameter $k$ is used as kernel function.
The means and covariances at a desired $x$ can then be estimated as
\begin{equation}\begin{split}
  \mu_x &= \frac{\sum_i h_ix_i}{n}, \quad
  \sigma_x^2 = \frac{\sum_i h_i(x_i-\mu_x)^2}{n}, \\
  \mu_y &= \frac{\sum_i h_iy_i}{n}, \quad
  \sigma_y^2 = \frac{\sum_i h_i(x_i-\mu_x)^2}{n}, \\
  \sigma_{xy} &= \frac{\sum_i h_i(x_i-\mu_x)(y_i-\mu_y)}{n}
\end{split}\end{equation}
with $n = \sum_i h_i$.
\end{frame}

%..............................................................................
\begin{frame}{Locally Weighted Regression (LWR)}
\footnotesize
Again, the learner's new variance at $x$ can be estimated as
\begin{equation}
\E{\st_{\hat{y}}^2} = \frac{\E{\st_{y|x}^2}}{(n+\hht)^2}
\left[ \sum_i h_i^2 + \hht^2 + \frac{(x-\mut_x)^2}{\st_x^2}
\left( \sum_i h_i^2 \frac{(x_i - \mut_x)^2}{\st_x^2} +
\hht^2 \frac{(\xt-\mut)^2}{\st_x^2} \right) \right]
\end{equation}
with

\tiny
\begin{columns}[t]
\begin{column}{0.45\textwidth}
\[\begin{split}
\E{\st_{y|x}^2} &= \E{\st_{y}^2} - \frac{\E{\st_{xy}^2}}{\sigma_{x}^2},\\
\E{\st_{xy}^2} &= \E{\st_{xy}}^2 +
\frac{n^2\hht^2\sigma_{y|\xt}^2(\xt-\mu_{x})^2}{(n+\hht)^4},\\
\sigma_{y|x}^2 &= \sigma_{y}^2 - \frac{\sigma_{xy}^2}{\sigma_{x}^2},\\
\mut_x &= \frac{n\mu_x+\hht\xt}{n+\hht},
\end{split}\]
\end{column}
\begin{column}{0.55\textwidth}
\[\begin{split}
\E{\st_{y}^2} &= \frac{n\sigma_{y}^2}{n+\hht} +
\frac{n\hht \left( \sigma_{y|\xt}^2 + (\hat{y}(\xt)-\mu_{y})^2 \right)}
{(n+\hht)^2}, \\
\E{\st_{xy}} &= \frac{n\sigma_{xy}}{n+\hht} +
\frac{n \hht (\xt-\mu_{x})(\hat{y}(\xt)-\mu_{y})}{(n+\hht)^2},\\
\hat{y}(x) &= \mu_{y} + \frac{\sigma_{xy}}{\sigma_{x}^2}(x-\mu_{x}),\\
\st_{x}^2 &= \frac{n\sigma_{x}^2}{n+\hht} +
\frac{n \hht (\xt-\mu_{x})^2}{(n+\hht)^2},\\
\end{split}\]
\end{column}
\end{columns}
\end{frame}

%%%_____________________________________________________________________________
%%%
%%%  Results
%%%_____________________________________________________________________________

\begin{frame}{Experimental Results - 2 DoF Robot Arm}
\begin{itemize}
\item task is to learn kinematics of 2 DoF robot arm
\item input: joint angles $(\Phi_1,\Phi_2)$
\item output: Cartesian coordinates $(X_1,X_2)$
\item additive Gaussian noise is applied on inputs, which simulates
  noisy arm effectors and joint angle sensors
\end{itemize}
Although both models implicitly assume Gaussian noise in the output dimensions,
this experiment results in non-Gaussian errors.
\end{frame}

%..............................................................................
\end{document}